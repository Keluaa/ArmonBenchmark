name=MPI_cmp_2D_GPU_16proc_a100
device=CUDA
node=a100
processes=16
node_count=4
threads=32
dim=2
create_sub_job_chain=true
add_reference_job=true
one_job_per_cell=true
jl_places=cores
jl_proc_bind=close
domains=640,640; 1200,1200; 1760,1760; 2400,2400; 6400,6400; 10000,10000; 20000,20000; 32000,32000; 44000,44000; 52000,52000; 60000,60000
process_grid_ratios=1,1
splitting=Sequential
repeats=3
tests=Sod_circ
armon=--verbose-mpi 0 --euler 1 --cycle 600 --single-comm 1 --nghost 3
log_scale=true
time_MPI_plot=true
title=2D Eulerian 2nd-order hydro with sub-domains, 600 cycles, 3 samples, cylindical Sod, 2.5 comms per cycle
max_time=14400
-
name=MPI_cmp_2D_CPU_16proc_a100
device=CPU
node=a100
processes=16
node_count=4
threads=32
dim=2
create_sub_job_chain=true
add_reference_job=true
one_job_per_cell=true
jl_places=cores
jl_proc_bind=close
domains=640,640; 1200,1200; 1760,1760; 2400,2400; 6400,6400; 10000,10000; 20000,20000; 32000,32000; 44000,44000; 52000,52000; 60000,60000
process_grid_ratios=1,1
splitting=Sequential
repeats=3
tests=Sod_circ
armon=--verbose-mpi 0 --euler 1 --cycle 600 --single-comm 1 --nghost 3
log_scale=true
time_MPI_plot=true
title=2D Eulerian 2nd-order hydro with sub-domains, 600 cycles, 3 samples, cylindical Sod, 2.5 comms per cycle
max_time=14400
